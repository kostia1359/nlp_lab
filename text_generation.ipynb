{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDJctDhhEDTD"
      },
      "source": [
        "## Importing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2xZnxncD2Ep"
      },
      "outputs": [],
      "source": [
        "# Selecting Tensorflow version v2 (the command is relevant for Colab only).\n",
        "# %tensorflow_version 2.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpueB6zADYgE",
        "outputId": "8dbe82b7-d2bb-4b90-f322-d930f3a2c655"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import platform\n",
        "import time\n",
        "import pathlib\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciI_JnnNEGCw"
      },
      "source": [
        "## Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vk2HldshEOph",
        "outputId": "db73ba42-ec55-42d3-c897-e216c9dd6c93"
      },
      "outputs": [],
      "source": [
        "cache_dir = './tmp'\n",
        "dataset_file_name = 'shakespeare.txt'\n",
        "dataset_file_origin = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
        "\n",
        "dataset_file_path = tf.keras.utils.get_file(\n",
        "    fname=dataset_file_name,\n",
        "    origin=dataset_file_origin,\n",
        "    cache_dir=pathlib.Path(cache_dir).absolute()\n",
        ")\n",
        "\n",
        "print(dataset_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKTy6YS5Gx-g"
      },
      "source": [
        "## Analyze the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzfX7hK8G0bn",
        "outputId": "bc106fb9-ce9c-43fb-dd21-72477ec773e0"
      },
      "outputs": [],
      "source": [
        "# Reading the database file.\n",
        "text = open(dataset_file_path, mode='r').read()\n",
        "\n",
        "print('Length of text: {} chars'.format(len(text)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nknOAOuoH-Av",
        "outputId": "e2799bbf-906e-4271-df8e-ab251d8f729d"
      },
      "outputs": [],
      "source": [
        "# First 250 characters in text.\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqpuKh9HMNnf"
      },
      "source": [
        "## Process the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dj4e-AGMaV4"
      },
      "source": [
        "### Vectorize the text\n",
        "\n",
        "Before feeding the text to our RNN we need to convert the text from a sequence of characters to a sequence of numbers. To do so we will detect all unique characters in the text, form a vocabulary out of it and replace each character with its index in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "xFFpuXfGMPq2",
        "outputId": "2dfb25d0-c249-40ff-f82c-5e82e8eaa0cb"
      },
      "outputs": [],
      "source": [
        "# Map characters to their indices in vocabulary.\n",
        "vocab = sorted(set(text))\n",
        "char2index = {char: index for index, char in enumerate(vocab)}\n",
        "\n",
        "print('{')\n",
        "for char, _ in zip(char2index, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2index[char]))\n",
        "print('  ...\\n}')\n",
        "\n",
        "index2char = np.array(vocab)\n",
        "text_as_int = np.array([char2index[char] for char in text])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHv5HhUuTQYS"
      },
      "source": [
        "## Create training sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tHT8IXGTTtZ",
        "outputId": "4ba55c64-b0f6-46f9-8de1-7b3ac9a55771"
      },
      "outputs": [],
      "source": [
        "# Printing examples per epoch\n",
        "sequence_length = 100\n",
        "examples_per_epoch = len(text) // (sequence_length + 1)\n",
        "\n",
        "print('examples_per_epoch:', examples_per_epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf5tMMDnUP3q",
        "outputId": "5fe18d40-f350-448c-b72d-b3bb2e3d2245"
      },
      "outputs": [],
      "source": [
        "# Create training dataset.\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for char in char_dataset.take(5):\n",
        "    print(index2char[char.numpy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap71VjB2Vuct",
        "outputId": "fd19fecc-2dda-42a6-bb90-1989a8f9a267"
      },
      "outputs": [],
      "source": [
        "# Generate batched sequences out of the char_dataset.\n",
        "sequences = char_dataset.batch(sequence_length + 1, drop_remainder=True)\n",
        "\n",
        "# Sequences size is the same as examples_per_epoch.\n",
        "print('Sequences count: {}'.format(len(list(sequences.as_numpy_iterator()))));\n",
        "print()\n",
        "\n",
        "# Sequences examples.\n",
        "for item in sequences.take(5):\n",
        "    print(repr(''.join(index2char[item.numpy()])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdcrcUs4Xxso"
      },
      "source": [
        "For each sequence, duplicate and shift it to form the input and target text. For example, say `sequence_length` is `4` and our text is `Hello`. The input sequence would be `Hell`, and the target sequence `ello`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fxvXsP0XFDh"
      },
      "outputs": [],
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "454rWIQYXXRY",
        "outputId": "75fd7cb8-99a7-4d7f-c7d4-cd93395a58c6"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Dataset size is the same as examples_per_epoch.\n",
        "# But each element of a sequence is now has length of `sequence_length`\n",
        "# and not `sequence_length + 1`.\n",
        "print('dataset size: {}'.format(len(list(dataset.as_numpy_iterator()))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kuoh4tCdYCck",
        "outputId": "60639502-b626-44a7-993c-f24a571a72bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input sequence size: 100\n",
            "Target sequence size: 100\n",
            "\n",
            "Input: 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print('Input sequence size:', repr(len(input_example.numpy())))\n",
        "    print('Target sequence size:', repr(len(target_example.numpy())))\n",
        "    print()\n",
        "    print('Input:', repr(''.join(index2char[input_example.numpy()])))\n",
        "    print('Target:', repr(''.join(index2char[target_example.numpy()])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDYHEJ0pY1ai"
      },
      "source": [
        "Each index of these vectors are processed as one time step. For the input at time step 0, the model receives the index for \"F\" and trys to predict the index for \"i\" as the next character. At the next timestep, it does the same thing but the RNN considers the previous step context in addition to the current input character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-0zpv53Y2o4",
        "outputId": "a27a0d56-a9d7-485d-e9af-1449bd2e820a"
      },
      "outputs": [],
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print('Step {:2d}'.format(i))\n",
        "    print('  input: {} ({:s})'.format(input_idx, repr(index2char[input_idx])))\n",
        "    print('  expected output: {} ({:s})'.format(target_idx, repr(index2char[target_idx])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iDlp40lC5YB"
      },
      "source": [
        "## Split training sequences into batches\n",
        "\n",
        "We used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, we need to shuffle the data and pack it into batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDq-wa5EC3wW",
        "outputId": "e327fb5c-8f30-4010-d454-be04b9e2ce32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Batch size.\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset (TF data is designed to work\n",
        "# with possibly infinite sequences, so it doesn't attempt to shuffle\n",
        "# the entire sequence in memory. Instead, it maintains a buffer in\n",
        "# which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1x4puZiiOlyl",
        "outputId": "3953f037-2727-41f4-90fe-55e6f4e36055"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batched dataset size: 172\n"
          ]
        }
      ],
      "source": [
        "print('Batched dataset size: {}'.format(len(list(dataset.as_numpy_iterator()))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_kYgvQGBO0U",
        "outputId": "ab903f7d-4786-44eb-8f77-1582ecc4ce29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1st batch: input_text: tf.Tensor(\n",
            "[[26 53 61 ... 47 52  1]\n",
            " [ 1 45 53 ... 47 57  1]\n",
            " [27 24 33 ... 56 58 46]\n",
            " ...\n",
            " [59 57  8 ... 60 43  8]\n",
            " [ 1 39 52 ... 50 47 52]\n",
            " [50  1 39 ... 39 41 43]], shape=(64, 100), dtype=int64)\n",
            "\n",
            "1st batch: target_text: tf.Tensor(\n",
            "[[53 61  1 ... 52  1 58]\n",
            " [45 53 53 ... 57  1 52]\n",
            " [24 33 25 ... 58 46  0]\n",
            " ...\n",
            " [57  8  0 ... 43  8  0]\n",
            " [39 52  1 ... 47 52 42]\n",
            " [ 1 39 52 ... 41 43  8]], shape=(64, 100), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "for input_text, target_text in dataset.take(1):\n",
        "    print('1st batch: input_text:', input_text)\n",
        "    print()\n",
        "    print('1st batch: target_text:', target_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghB-VwLlD-Oz"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "Use [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) to define the model. For this simple example three layers are used to define our model:\n",
        "\n",
        "- [tf.keras.layers.Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding): The input layer. A trainable lookup table that will map the numbers of each character to a vector with `embedding_dim` dimensions;\n",
        "- [tf.keras.layers.LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM): A type of RNN with size units=rnn_units (You can also use a GRU layer here.)\n",
        "- [tf.keras.layers.Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense): The output layer, with vocab_size outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cg8DlO3QjuT",
        "outputId": "116cbc5f-f792-499a-afb0-9cb90500a55f"
      },
      "outputs": [],
      "source": [
        "# Let's do a quick detour and see how Embeding layer works.\n",
        "# It takes several char indices sequences (batch) as an input.\n",
        "# It encodes every character of every sequence to a vector of tmp_embeding_size length.\n",
        "tmp_vocab_size = 10\n",
        "tmp_embeding_size = 5\n",
        "tmp_input_length = 8\n",
        "tmp_batch_size = 2\n",
        "\n",
        "tmp_model = tf.keras.models.Sequential()\n",
        "tmp_model.add(tf.keras.layers.Embedding(\n",
        "  input_dim=tmp_vocab_size,\n",
        "  output_dim=tmp_embeding_size,\n",
        "  input_length=tmp_input_length\n",
        "))\n",
        "# The model will take as input an integer matrix of size (batch, input_length).\n",
        "# The largest integer (i.e. word index) in the input should be no larger than 9 (tmp_vocab_size).\n",
        "# Now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
        "tmp_input_array = np.random.randint(\n",
        "  low=0,\n",
        "  high=tmp_vocab_size,\n",
        "  size=(tmp_batch_size, tmp_input_length)\n",
        ")\n",
        "tmp_model.compile('rmsprop', 'mse')\n",
        "tmp_output_array = tmp_model.predict(tmp_input_array)\n",
        "\n",
        "print('tmp_input_array shape:', tmp_input_array.shape)\n",
        "print('tmp_input_array:')\n",
        "print(tmp_input_array)\n",
        "print()\n",
        "print('tmp_output_array shape:', tmp_output_array.shape)\n",
        "print('tmp_output_array:')\n",
        "print(tmp_output_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7ZuvZHBD_pS"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in chars.\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension.\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units.\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sojdDCAICWO"
      },
      "outputs": [],
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    model.add(tf.keras.layers.Embedding(\n",
        "      input_dim=vocab_size,\n",
        "      output_dim=embedding_dim,\n",
        "      batch_input_shape=[batch_size, None]\n",
        "    ))\n",
        "\n",
        "    model.add(tf.keras.layers.LSTM(\n",
        "      units=rnn_units,\n",
        "      return_sequences=True,\n",
        "      stateful=True,\n",
        "      recurrent_initializer=tf.keras.initializers.GlorotNormal()\n",
        "    ))\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(vocab_size))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoPwxyAPEg6z"
      },
      "outputs": [],
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLnlZFgU55bQ",
        "outputId": "4a438b58-2f8f-4b36-f376-9823d6a436ff"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "CcaO_rO_8-GH",
        "outputId": "f5f175a1-5180-46e2-81e2-8dec217687bf"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model,\n",
        "    show_shapes=True,\n",
        "    show_layer_names=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Npruiy2RAPkt"
      },
      "source": [
        "## Try the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4DCLA0GASL1",
        "outputId": "d59f7209-a81b-4eae-f448-a21f394750b0"
      },
      "outputs": [],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWebJXU9CEPd"
      },
      "source": [
        "To get actual predictions from the model we need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4Jgo-iECFWI",
        "outputId": "874f2a08-0ac9-4417-9501-303b26eb7f3d"
      },
      "outputs": [],
      "source": [
        "print('Prediction for the 1st letter of the batch 1st sequense:')\n",
        "print(example_batch_predictions[0, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dOr0MwFHlRb",
        "outputId": "67c2d834-2515-4776-e31f-69c08b16836e"
      },
      "outputs": [],
      "source": [
        "# Quick overview of how tf.random.categorical() works.\n",
        "\n",
        "# logits is 2-D Tensor with shape [batch_size, num_classes].\n",
        "# Each slice [i, :] represents the unnormalized log-probabilities for all classes.\n",
        "# In the example below we say that the probability for class \"0\" is low but the\n",
        "# probability for class \"2\" is much higher.\n",
        "tmp_logits = [\n",
        "  [-0.95, 0, 0.95],\n",
        "];\n",
        "\n",
        "# Let's generate 5 samples. Each sample is a class index. Class probabilities\n",
        "# are being taken into account (we expect to see more samples of class \"2\").\n",
        "tmp_samples = tf.random.categorical(\n",
        "    logits=tmp_logits,\n",
        "    num_samples=5\n",
        ")\n",
        "\n",
        "print(tmp_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPzr0r4zCgS3",
        "outputId": "c4748980-cb99-4f68-fde6-a0664d55b598"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(\n",
        "    logits=example_batch_predictions[0],\n",
        "    num_samples=1\n",
        ")\n",
        "\n",
        "sampled_indices.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaA7DclID8dz",
        "outputId": "5f082dfe-2319-4c57-a261-490a87164c4f"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.squeeze(\n",
        "    input=sampled_indices,\n",
        "    axis=-1\n",
        ").numpy()\n",
        "\n",
        "sampled_indices.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ubGQ0gVENhB",
        "outputId": "d47796b9-9564-4fb7-8d40-e86ce91e1f80"
      },
      "outputs": [],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi9HOzw9EajS",
        "outputId": "9a5bf5b2-42b9-47f4-e04d-72f7fa9bee86"
      },
      "outputs": [],
      "source": [
        "print('Input:\\n', repr(''.join(index2char[input_example_batch[0]])))\n",
        "print()\n",
        "print('Next char prediction:\\n', repr(''.join(index2char[sampled_indices])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b87e0lsYMTsv",
        "outputId": "4ac3496b-1236-401b-fe54-aac573894044"
      },
      "outputs": [],
      "source": [
        "for i, (input_idx, sample_idx) in enumerate(zip(input_example_batch[0][:5], sampled_indices[:5])):\n",
        "    print('Prediction {:2d}'.format(i))\n",
        "    print('  input: {} ({:s})'.format(input_idx, repr(index2char[input_idx])))\n",
        "    print('  next predicted: {} ({:s})'.format(target_idx, repr(index2char[sample_idx])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqcBufKEE_p6"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4s0-PvrFub5"
      },
      "source": [
        "### Attach an optimizer, and a loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOEUUm6JE95a",
        "outputId": "815f7289-c694-46cd-a671-bd65b8909dee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.173165\n"
          ]
        }
      ],
      "source": [
        "# An objective function.\n",
        "# The function is any callable with the signature scalar_loss = fn(y_true, y_pred).\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(\n",
        "      y_true=labels,\n",
        "      y_pred=logits,\n",
        "      from_logits=True\n",
        "    )\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXhJsB6eFgrJ"
      },
      "outputs": [],
      "source": [
        "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(\n",
        "    optimizer=adam_optimizer,\n",
        "    loss=loss\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK3Cf-xZFwL4"
      },
      "source": [
        "### Configure checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUhXnHPJFy5q"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved.\n",
        "checkpoint_dir = 'tmp/checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFg9MFJoGZWf"
      },
      "source": [
        "### Execute the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVk-pARPGaja"
      },
      "outputs": [],
      "source": [
        "EPOCHS=300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0rveBdAGeEz",
        "outputId": "d4243b8d-063c-43ae-e81a-5609ac160767"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "172/172 [==============================] - 16s 74ms/step - loss: 2.7470\n",
            "Epoch 2/300\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 2.0946\n",
            "Epoch 3/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 1.8432\n",
            "Epoch 4/300\n",
            "172/172 [==============================] - 16s 77ms/step - loss: 1.6757\n",
            "Epoch 5/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 1.5629\n",
            "Epoch 6/300\n",
            "172/172 [==============================] - 15s 74ms/step - loss: 1.4846\n",
            "Epoch 7/300\n",
            "172/172 [==============================] - 16s 74ms/step - loss: 1.4282\n",
            "Epoch 8/300\n",
            "172/172 [==============================] - 15s 74ms/step - loss: 1.3828\n",
            "Epoch 9/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 1.3458\n",
            "Epoch 10/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 1.3143\n",
            "Epoch 11/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 1.2839\n",
            "Epoch 12/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 1.2558\n",
            "Epoch 13/300\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 1.2275\n",
            "Epoch 14/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 1.1984\n",
            "Epoch 15/300\n",
            "172/172 [==============================] - 15s 74ms/step - loss: 1.1682\n",
            "Epoch 16/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 1.1375\n",
            "Epoch 17/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 1.1048\n",
            "Epoch 18/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 1.0699\n",
            "Epoch 19/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 1.0337\n",
            "Epoch 20/300\n",
            "172/172 [==============================] - 15s 74ms/step - loss: 0.9971\n",
            "Epoch 21/300\n",
            "172/172 [==============================] - 16s 74ms/step - loss: 0.9591\n",
            "Epoch 22/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.9203\n",
            "Epoch 23/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.8821\n",
            "Epoch 24/300\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.8448\n",
            "Epoch 25/300\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.8096\n",
            "Epoch 26/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.7742\n",
            "Epoch 27/300\n",
            "172/172 [==============================] - 16s 77ms/step - loss: 0.7417\n",
            "Epoch 28/300\n",
            "172/172 [==============================] - 16s 76ms/step - loss: 0.7118\n",
            "Epoch 29/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.6841\n",
            "Epoch 30/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.6571\n",
            "Epoch 31/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.6345\n",
            "Epoch 32/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.6104\n",
            "Epoch 33/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.5912\n",
            "Epoch 34/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.5735\n",
            "Epoch 35/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.5574\n",
            "Epoch 36/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.5429\n",
            "Epoch 37/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.5292\n",
            "Epoch 38/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.5168\n",
            "Epoch 39/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.5053\n",
            "Epoch 40/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.4945\n",
            "Epoch 41/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.4857\n",
            "Epoch 42/300\n",
            "172/172 [==============================] - 16s 75ms/step - loss: 0.4781\n",
            "Epoch 43/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.4688\n",
            "Epoch 44/300\n",
            "172/172 [==============================] - 15s 74ms/step - loss: 0.4629\n",
            "Epoch 45/300\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.4552\n",
            "Epoch 46/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.4501\n",
            "Epoch 47/300\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.4434\n",
            "Epoch 48/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.4386\n",
            "Epoch 49/300\n",
            "172/172 [==============================] - 16s 76ms/step - loss: 0.4332\n",
            "Epoch 50/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.4296\n",
            "Epoch 51/300\n",
            "172/172 [==============================] - 15s 74ms/step - loss: 0.4250\n",
            "Epoch 52/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.4227\n",
            "Epoch 53/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.4186\n",
            "Epoch 54/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.4148\n",
            "Epoch 55/300\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.4120\n",
            "Epoch 56/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.4078\n",
            "Epoch 57/300\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.4062\n",
            "Epoch 58/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.4026\n",
            "Epoch 59/300\n",
            "172/172 [==============================] - 15s 73ms/step - loss: 0.4006\n",
            "Epoch 60/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.3996\n",
            "Epoch 61/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3964\n",
            "Epoch 62/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.3955\n",
            "Epoch 63/300\n",
            "172/172 [==============================] - 15s 73ms/step - loss: 0.3921\n",
            "Epoch 64/300\n",
            "172/172 [==============================] - 16s 76ms/step - loss: 0.3911\n",
            "Epoch 65/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3888\n",
            "Epoch 66/300\n",
            "172/172 [==============================] - 16s 75ms/step - loss: 0.3860\n",
            "Epoch 67/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.3837\n",
            "Epoch 68/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.3832\n",
            "Epoch 69/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.3828\n",
            "Epoch 70/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3802\n",
            "Epoch 71/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3763\n",
            "Epoch 72/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3756\n",
            "Epoch 73/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.3754\n",
            "Epoch 74/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3758\n",
            "Epoch 75/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3746\n",
            "Epoch 76/300\n",
            "172/172 [==============================] - 15s 74ms/step - loss: 0.3724\n",
            "Epoch 77/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3710\n",
            "Epoch 78/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3685\n",
            "Epoch 79/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3677\n",
            "Epoch 80/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3673\n",
            "Epoch 81/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.3687\n",
            "Epoch 82/300\n",
            "172/172 [==============================] - 15s 73ms/step - loss: 0.3666\n",
            "Epoch 83/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.3637\n",
            "Epoch 84/300\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.3599\n",
            "Epoch 85/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3609\n",
            "Epoch 86/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3621\n",
            "Epoch 87/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3581\n",
            "Epoch 88/300\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.3604\n",
            "Epoch 89/300\n",
            "172/172 [==============================] - 16s 77ms/step - loss: 0.3578\n",
            "Epoch 90/300\n",
            "172/172 [==============================] - 16s 76ms/step - loss: 0.3590\n",
            "Epoch 91/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3556\n",
            "Epoch 92/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.3556\n",
            "Epoch 93/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3554\n",
            "Epoch 94/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3560\n",
            "Epoch 95/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3535\n",
            "Epoch 96/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3529\n",
            "Epoch 97/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3541\n",
            "Epoch 98/300\n",
            "172/172 [==============================] - 16s 75ms/step - loss: 0.3538\n",
            "Epoch 99/300\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.3530\n",
            "Epoch 100/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3510\n",
            "Epoch 101/300\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.3525\n",
            "Epoch 102/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.3491\n",
            "Epoch 103/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3452\n",
            "Epoch 104/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3468\n",
            "Epoch 105/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.3453\n",
            "Epoch 106/300\n",
            "172/172 [==============================] - 16s 74ms/step - loss: 0.3443\n",
            "Epoch 107/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3450\n",
            "Epoch 108/300\n",
            "172/172 [==============================] - 16s 76ms/step - loss: 0.3452\n",
            "Epoch 109/300\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.3462\n",
            "Epoch 110/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.3446\n",
            "Epoch 111/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3443\n",
            "Epoch 112/300\n",
            "172/172 [==============================] - 15s 73ms/step - loss: 0.3422\n",
            "Epoch 113/300\n",
            "172/172 [==============================] - 16s 76ms/step - loss: 0.3401\n",
            "Epoch 114/300\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.3405\n",
            "Epoch 115/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3416\n",
            "Epoch 116/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3432\n",
            "Epoch 117/300\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.3437\n",
            "Epoch 118/300\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.3437\n",
            "Epoch 119/300\n",
            "172/172 [==============================] - 16s 74ms/step - loss: 0.3415\n",
            "Epoch 120/300\n",
            "172/172 [==============================] - 16s 76ms/step - loss: 0.3419\n",
            "Epoch 121/300\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.3374\n",
            "Epoch 122/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.3362\n",
            "Epoch 123/300\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.3376\n",
            "Epoch 124/300\n",
            "172/172 [==============================] - 15s 73ms/step - loss: 0.3363\n",
            "Epoch 125/300\n",
            "172/172 [==============================] - 16s 75ms/step - loss: 0.3341\n",
            "Epoch 126/300\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.3341\n",
            "Epoch 127/300\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.3332\n",
            "Epoch 128/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.3366\n",
            "Epoch 129/300\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.3356\n",
            "Epoch 130/300\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.3363\n",
            "Epoch 131/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3364\n",
            "Epoch 132/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3356\n",
            "Epoch 133/300\n",
            "172/172 [==============================] - 16s 75ms/step - loss: 0.3342\n",
            "Epoch 134/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3318\n",
            "Epoch 135/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3335\n",
            "Epoch 136/300\n",
            "172/172 [==============================] - 15s 74ms/step - loss: 0.3329\n",
            "Epoch 137/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3319\n",
            "Epoch 138/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3312\n",
            "Epoch 139/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3292\n",
            "Epoch 140/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3299\n",
            "Epoch 141/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3307\n",
            "Epoch 142/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.3275\n",
            "Epoch 143/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.3295\n",
            "Epoch 144/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3321\n",
            "Epoch 145/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3338\n",
            "Epoch 146/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3327\n",
            "Epoch 147/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3316\n",
            "Epoch 148/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.3290\n",
            "Epoch 149/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3282\n",
            "Epoch 150/300\n",
            "172/172 [==============================] - 16s 75ms/step - loss: 0.3282\n",
            "Epoch 151/300\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.3296\n",
            "Epoch 152/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3292\n",
            "Epoch 153/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.3259\n",
            "Epoch 154/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3246\n",
            "Epoch 155/300\n",
            "172/172 [==============================] - 15s 74ms/step - loss: 0.3238\n",
            "Epoch 156/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3245\n",
            "Epoch 157/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3270\n",
            "Epoch 158/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3293\n",
            "Epoch 159/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.3299\n",
            "Epoch 160/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3273\n",
            "Epoch 161/300\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.3254\n",
            "Epoch 162/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3238\n",
            "Epoch 163/300\n",
            "172/172 [==============================] - 16s 78ms/step - loss: 0.3248\n",
            "Epoch 164/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3208\n",
            "Epoch 165/300\n",
            "172/172 [==============================] - 16s 74ms/step - loss: 0.3212\n",
            "Epoch 166/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3212\n",
            "Epoch 167/300\n",
            "172/172 [==============================] - 16s 76ms/step - loss: 0.3226\n",
            "Epoch 168/300\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.3249\n",
            "Epoch 169/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3272\n",
            "Epoch 170/300\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.3284\n",
            "Epoch 171/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.3280\n",
            "Epoch 172/300\n",
            "172/172 [==============================] - 16s 77ms/step - loss: 0.3255\n",
            "Epoch 173/300\n",
            "172/172 [==============================] - 16s 77ms/step - loss: 0.3248\n",
            "Epoch 174/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3252\n",
            "Epoch 175/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3238\n",
            "Epoch 176/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.3208\n",
            "Epoch 177/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.3197\n",
            "Epoch 178/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3169\n",
            "Epoch 179/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.3212\n",
            "Epoch 180/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3245\n",
            "Epoch 181/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3235\n",
            "Epoch 182/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3253\n",
            "Epoch 183/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.3215\n",
            "Epoch 184/300\n",
            "172/172 [==============================] - 15s 79ms/step - loss: 0.3223\n",
            "Epoch 185/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.3228\n",
            "Epoch 186/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3214\n",
            "Epoch 187/300\n",
            "172/172 [==============================] - 16s 76ms/step - loss: 0.3182\n",
            "Epoch 188/300\n",
            "172/172 [==============================] - 16s 78ms/step - loss: 0.3190\n",
            "Epoch 189/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3208\n",
            "Epoch 190/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.3185\n",
            "Epoch 191/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.3214\n",
            "Epoch 192/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.3207\n",
            "Epoch 193/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.3210\n",
            "Epoch 194/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3196\n",
            "Epoch 195/300\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.3173\n",
            "Epoch 196/300\n",
            "172/172 [==============================] - 15s 74ms/step - loss: 0.3193\n",
            "Epoch 197/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3183\n",
            "Epoch 198/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3195\n",
            "Epoch 199/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.3181\n",
            "Epoch 200/300\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.3164\n",
            "Epoch 201/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3174\n",
            "Epoch 202/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.3202\n",
            "Epoch 203/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3172\n",
            "Epoch 204/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3173\n",
            "Epoch 205/300\n",
            "172/172 [==============================] - 15s 74ms/step - loss: 0.3176\n",
            "Epoch 206/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3187\n",
            "Epoch 207/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.3196\n",
            "Epoch 208/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3182\n",
            "Epoch 209/300\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.3157\n",
            "Epoch 210/300\n",
            "172/172 [==============================] - 16s 76ms/step - loss: 0.3210\n",
            "Epoch 211/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3238\n",
            "Epoch 212/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3206\n",
            "Epoch 213/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3181\n",
            "Epoch 214/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3169\n",
            "Epoch 215/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3164\n",
            "Epoch 216/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3142\n",
            "Epoch 217/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3126\n",
            "Epoch 218/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3138\n",
            "Epoch 219/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3158\n",
            "Epoch 220/300\n",
            "172/172 [==============================] - 15s 73ms/step - loss: 0.3220\n",
            "Epoch 221/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3223\n",
            "Epoch 222/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3263\n",
            "Epoch 223/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.3242\n",
            "Epoch 224/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3203\n",
            "Epoch 225/300\n",
            "172/172 [==============================] - 15s 74ms/step - loss: 0.3177\n",
            "Epoch 226/300\n",
            "172/172 [==============================] - 16s 77ms/step - loss: 0.3114\n",
            "Epoch 227/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3103\n",
            "Epoch 228/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3100\n",
            "Epoch 229/300\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.3111\n",
            "Epoch 230/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.3111\n",
            "Epoch 231/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3140\n",
            "Epoch 232/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3184\n",
            "Epoch 233/300\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.3248\n",
            "Epoch 234/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3268\n",
            "Epoch 235/300\n",
            "172/172 [==============================] - 15s 74ms/step - loss: 0.3247\n",
            "Epoch 236/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3238\n",
            "Epoch 237/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3210\n",
            "Epoch 238/300\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.3171\n",
            "Epoch 239/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.3154\n",
            "Epoch 240/300\n",
            "172/172 [==============================] - 16s 76ms/step - loss: 0.3122\n",
            "Epoch 241/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3100\n",
            "Epoch 242/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3076\n",
            "Epoch 243/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.3101\n",
            "Epoch 244/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3117\n",
            "Epoch 245/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3135\n",
            "Epoch 246/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3143\n",
            "Epoch 247/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.3152\n",
            "Epoch 248/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3159\n",
            "Epoch 249/300\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.3205\n",
            "Epoch 250/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.3212\n",
            "Epoch 251/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3242\n",
            "Epoch 252/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3258\n",
            "Epoch 253/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3240\n",
            "Epoch 254/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3230\n",
            "Epoch 255/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3179\n",
            "Epoch 256/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.3134\n",
            "Epoch 257/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3107\n",
            "Epoch 258/300\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.3050\n",
            "Epoch 259/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3048\n",
            "Epoch 260/300\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.3086\n",
            "Epoch 261/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.3118\n",
            "Epoch 262/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3177\n",
            "Epoch 263/300\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.3189\n",
            "Epoch 264/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.3231\n",
            "Epoch 265/300\n",
            "172/172 [==============================] - 15s 78ms/step - loss: 0.3240\n",
            "Epoch 266/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3239\n",
            "Epoch 267/300\n",
            "172/172 [==============================] - 15s 74ms/step - loss: 0.3220\n",
            "Epoch 268/300\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.3195\n",
            "Epoch 269/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.3168\n",
            "Epoch 270/300\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.3119\n",
            "Epoch 271/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.3095\n",
            "Epoch 272/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3053\n",
            "Epoch 273/300\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.3040\n",
            "Epoch 274/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3042\n",
            "Epoch 275/300\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.3064\n",
            "Epoch 276/300\n",
            "172/172 [==============================] - 15s 74ms/step - loss: 0.3082\n",
            "Epoch 277/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3140\n",
            "Epoch 278/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3222\n",
            "Epoch 279/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3296\n",
            "Epoch 280/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.3297\n",
            "Epoch 281/300\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.3275\n",
            "Epoch 282/300\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.3205\n",
            "Epoch 283/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3228\n",
            "Epoch 284/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3171\n",
            "Epoch 285/300\n",
            "172/172 [==============================] - 15s 74ms/step - loss: 0.3116\n",
            "Epoch 286/300\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.3135\n",
            "Epoch 287/300\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.3118\n",
            "Epoch 288/300\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3108\n",
            "Epoch 289/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3096\n",
            "Epoch 290/300\n",
            "172/172 [==============================] - 14s 72ms/step - loss: 0.3097\n",
            "Epoch 291/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.3100\n",
            "Epoch 292/300\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.3113\n",
            "Epoch 293/300\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.3129\n",
            "Epoch 294/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3147\n",
            "Epoch 295/300\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.3148\n",
            "Epoch 296/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.3200\n",
            "Epoch 297/300\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.3245\n",
            "Epoch 298/300\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 0.3242\n",
            "Epoch 299/300\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 0.3320\n",
            "Epoch 300/300\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.3263\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "  x=dataset,\n",
        "  epochs=EPOCHS,\n",
        "  callbacks=[\n",
        "    checkpoint_callback\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLdnOyvzMggJ"
      },
      "outputs": [],
      "source": [
        "def render_training_history(training_history):\n",
        "    loss = training_history.history['loss']\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(loss, label='Training set')\n",
        "    plt.legend()\n",
        "    plt.grid(linestyle='--', linewidth=1, alpha=0.5)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "4Ghveem_OQBV",
        "outputId": "3a86ccbb-d287-49e9-f045-8b983e533f70"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "render_training_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-dhNP2OG2EM"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU_YfP6sG3NC"
      },
      "source": [
        "### Restore the latest checkpoint\n",
        "\n",
        "To keep this prediction step simple, use a batch size of 1.\n",
        "\n",
        "Because of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built.\n",
        "\n",
        "To run the model with a different `batch_size`, we need to rebuild the model and restore the weights from the checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "BlG8o3wiG6f2",
        "outputId": "fb868046-1a01-4dcf-f7b6-f2f99264b431"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'tmp/checkpoints/ckpt_300'"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7evN0LvH01P"
      },
      "outputs": [],
      "source": [
        "simplified_batch_size = 1\n",
        "\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([simplified_batch_size, None]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3eduDtZI9zQ",
        "outputId": "8f8da970-d22b-4c00-b1a1-f9d041f33328"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOqdqGouJFf_"
      },
      "outputs": [],
      "source": [
        "# num_generate\n",
        "# - number of characters to generate.\n",
        "#\n",
        "# temperature\n",
        "# - Low temperatures results in more predictable text.\n",
        "# - Higher temperatures results in more surprising text.\n",
        "# - Experiment to find the best setting.\n",
        "def generate_text(model, start_string, num_generate = 1000, temperature=1.0):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing).\n",
        "    input_indices = [char2index[s] for s in start_string]\n",
        "    input_indices = tf.expand_dims(input_indices, 0)\n",
        "\n",
        "    # Empty string to store our results.\n",
        "    text_generated = []\n",
        "\n",
        "    # Here batch size == 1.\n",
        "    model.reset_states()\n",
        "    for char_index in range(num_generate):\n",
        "        predictions = model(input_indices)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # Using a categorical distribution to predict the character returned by the model.\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(\n",
        "        predictions,\n",
        "        num_samples=1\n",
        "        )[-1,0].numpy()\n",
        "\n",
        "        # We pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state.\n",
        "        input_indices = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(index2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-8e8P60J5Pg",
        "outputId": "fa4b58c7-180c-4bfd-d3c0-679451d06135"
      },
      "outputs": [],
      "source": [
        "# Generate the text with default temperature (1.0).\n",
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "wq_NlwWJSdix",
        "outputId": "304a5d66-85bf-43cd-e1a5-4aa2593c5e63"
      },
      "outputs": [],
      "source": [
        "# Generate the text with higher temperature to get more unexpected results.\n",
        "print(generate_text(model, start_string=u\"ROMEO: I love you\", num_generate = 100 ,temperature=0.1))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
